---
title: "R_HW_5_Author attribution"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages('tm')
#install.packages('tidyverse')
#install.packages('slam')
#install.packages('proxy')
```

```{r}
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
```

```{r}
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

```{r}
## Rolling 50 directories together into a single corpus
author_dirs = Sys.glob('ReutersC50/C50train/*')
author_dirs = author_dirs[1:50]
file_list = NULL
labels = NULL
for(author in author_dirs) {
	author_name = substring(author, first=29)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
```

```{r}
## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
documents_raw = Corpus(VectorSource(all_docs))

## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space
```

```{r}
## Remove stopwords.
# let's just use the "basic English" stop words
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
```

```{r}
## create a doc-term-matrix from the corpus
DTM = DocumentTermMatrix(my_documents)
DTM # some basic summary statistics
```

```{r}
## You can inspect its entries.
inspect(DTM[1:10,1:20])
```
```{r}
## Finally, let's drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
## can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.

DTM1 = removeSparseTerms(DTM, 0.975) #From 32570 to 1600
DTM1
```

```{r}
# construct TF IDF weights -- might be useful as features in a predictive model
tfidf = weightTfIdf(DTM1)
```

```{r}
# PCA on the TF-IDF weights
X = as.matrix(tfidf)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]

pca_congress=prcomp(X, scale=TRUE)
pve = summary(pc_congress)$importance[3,]
plot(pve)  # not much of an elbow
```

```{r}
######
# Principal component regression using PCs
######

# Construct the feature matrix and response vector.
# We'll take as candidate variables the leading PCs.
# Here TFIDF + PCA + truncating the lower-order (noisier) PCs
# is our "feature engineering" pipeline.
X = pca_congress$x[,1:100]
y = {memberdata$party == 'R'}
```


